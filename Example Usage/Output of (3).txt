> consensus_model=SUBSTRA.train_consensus(data = data,labels = labels,ntimes = 5,parallel = T,ncores = 5,verbose = T,pathToSUBSTRA = pathToSUBSTRA,
+                                         magnitudes = c(0.1,1,10),nfolds_tune = 3,phase_1_ite_tune = 45,phase_2_ite_tune = 15,parallel_tune = T,
+                                         phase_1_ite_train = 50,phase_2_ite_train = 20)
[1] "Tuning the 'magnitude'..."
[1] "Selected 'magnitude': 1"
[1] "Training the individual models..."
[1] "Aggregating the models..."
[1] "Cophenetic Coefficient for Column Clustering:"
[1] 0.9189055
[1] "Cophenetic Coefficient for Row Clustering:"
[1] 0.739345

> print(consensus_model)
$column.clusters
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 3 2 2 3 4 2 2 2 2 2 2 2 3 3 2 3 2 2 2 3 2 2 2 2 5 5 5 5 5 5
 [67] 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 4 6 6 6 4 6 4 4 4 4 4 6 3 6 7 6 2 3 4

$row.clusters
  [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 4 4 4 4 4 3 4 3 4 4 4 4 5 3 3 4 4 5 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4
 [67] 5 4 3 4 4 4 3 4 4 4 3 4 5 4 4 3 4 4 4 4 3 4 4 4 4 4 4 4 6 4 4 4 4 4 4 4 3 4 5 4 4 3 4 4 4 4 4 4 4 4 4 7 4 3 3 4 4 4 4 4 4 5 4 4 4 3
[133] 4 4 7 4 5 4 4 4 4 4 4 4 3 3 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4
[199] 4 4

$row.weights
  [1] 1.6637267 0.8028502 0.9327303 1.8995462 0.8745845 2.4100307 0.8412300 1.4035865 0.7142555 0.9406407 2.6343608 1.8687691 3.4004285
 [14] 1.1123090 1.1329993 0.4276488 2.8724121 0.3090224 3.4198780 1.2545409 0.9195150 1.0318863 0.8859176 0.9184130 0.9481901 1.0602496
 [27] 1.0386166 0.9958934 0.8884788 1.1489786 1.0960908 0.9486522 0.8521634 0.8423313 1.0131683 1.0263574 0.9242599 0.9734027 0.8617355
 [40] 0.9128165 0.9993017 0.8993196 1.0279191 0.9468865 1.0043253 1.0544026 0.7923047 0.9605331 0.9346201 0.8693528 0.8848926 0.9668551
 [53] 0.7067005 0.8119776 0.9373987 1.0777255 0.7964519 0.9135704 0.9831220 0.9029957 0.9922726 1.0354328 0.8581445 0.9085151 0.8683464
 [66] 0.8897723 0.9515572 0.9386096 0.9625876 0.8872825 0.7188798 1.1047493 1.0521813 0.9485943 1.0422488 0.7633866 0.8330157 1.0385317
 [79] 0.9285272 0.9877684 1.1733182 1.0354599 1.0335756 1.0062135 1.1565001 0.9470386 0.9197250 0.8724223 0.8580534 1.0133878 1.0189752
 [92] 1.0128014 1.0610625 1.0133309 1.0652166 1.0604339 1.0625160 0.9382177 0.9059637 0.9567760 0.9131004 1.2076284 0.9356371 1.0762419
[105] 0.8971579 0.9650487 0.9841481 0.9087932 0.9177771 0.9448681 0.9064659 1.0292378 1.0313311 0.9953789 0.9289424 1.0254253 0.9718117
[118] 1.1351307 0.8775960 1.1358978 0.8891512 0.9913633 1.0412343 0.8583275 0.9571120 0.8054323 1.0023406 1.0268392 1.0001773 0.9694649
[131] 1.1303572 0.9929149 0.9834901 0.8384258 1.2880417 1.0141053 0.8693436 1.1088577 0.9667887 1.0253392 0.9730716 1.0524008 1.0512806
[144] 1.0251090 0.9290821 0.7377003 1.0495895 0.9662253 1.0124160 0.9144436 0.9354913 0.9590273 1.0704560 1.0678281 0.8440996 1.0444482
[157] 0.9093397 1.0233767 0.9904064 0.9533724 0.9946072 1.0359384 1.0223387 0.8969886 0.9845464 0.8849084 1.0231882 0.8349263 0.9405423
[170] 1.1058741 1.1950098 1.0291348 0.8745424 0.8616700 0.9150431 0.9745420 0.9846465 1.0776881 0.7887555 0.9300542 1.0856277 0.8861023
[183] 0.9137370 0.9522674 1.0228853 1.0277248 0.8757691 1.0775463 0.9781889 1.0770696 1.0143634 1.0158316 0.9825094 0.9820135 1.1154726
[196] 0.8766884 1.1157250 0.7708963 1.0518109 1.1810163